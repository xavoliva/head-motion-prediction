# Weekly Log

## Week 1 (28/06 - 05/07)

* Read papers:

  * "Shooting a Moving Target: Motion-Prediction-Based Transmission for 360-Degree Videos", Bao et al.
  * "Your Attention is Unique: Detecting 360-Degree Video Saliency in Head-Mounted Display for Head Movement Prediction", Yan et al.
  * "Fixation Prediction for 360° Video Streaming in Head-Mounted Virtual Reality", Fan et al.
  * "Very Long Term Field of View Prediction for 360-degree Video Streaming", Li et al.
  * "Gaze Prediction in Dynamic 360° Immersive Videos", Xu et al.
  * "The prediction of head and eye movement for 360-degree images", Zu et al.
  * "Predicting Head Movement in Panoramic Video: A Deep Reinforcement Learning Approach", Xu et al.
  * "How do people explore virtual environments", Sitzmann et al.
  * "PREDICTING HEAD TRAJECTORIES IN 360°VIRTUAL REALITY VIDEOS", Aladagli et al.

* started reading book:
  * "Deep learning with Python", François Chollet: introduction to Keras library

* webcontroller GUI in browser works, can subscribe to topics

* TODO:
  * Decide, what I want to test and why
  * Set up Gitlab
  * Successfully launch ROS webcontroller (subscribe to & publish topics via webbrowser)
  * Check out Gazebo mono camera rig model
  * "Project plan"

* Meeting notes:
  * Meeting once per week, but flexible regarding the day of the week
  * Milestones:

  * July: implementation
  * August: first results * start writing, mid-August only write

* Mid-presentation: to be discussed

## Week 2 (5/07 - 12/07)

* Finished the book "Deep Learning in Python"
* Implemented an example LSTM network for temperature prediction (jena climate dataset)
* Imported data from our dataset and started preprocessing
* Set up and got used to a new environment: docker, jupyter notebook, ssh connection

* TODO:
  * Keras online course (edx, coursera)
  * Decide model parameters for LSTM network
  * Thoughts about early/late fusion
  * Import and preprocess data, interpolation when confidence == 0
  * Post processing of dataset, add column for cuts
  * Implement naive prediction (prediction == actual head orientation) and plot MAE and RMSE
  * Implement first LMST with only one input (Head orientation), plot MAE and RMSE

* Issues:

* CudNNLSTM does not work, still have to investigate why

* Meeting notes:
  * Prediction delay: 10 steps, from 0.1s to 1s
  * Loss functions:
    * MAE: prediction distance to ground truth
    * RMSE: smoothness of prediction

* Direction of Bachelor's thesis:
  * Head Motion Prediction using Saliency and Motion Maps
  * Features to be included:
    * Sensors: Orientation (x/y or quaternion), Gaze Position
    * Content: Saliency Map, Motion Map
  * Idea: compare early fusion: winner takes it all, mean between peaks, and late fusion: FoV and panoramic saliency map (equator bias?)

## Week 3 (12/07 - 19/07)

* Solved CuDNN issue, set up environment in new workstation (python 3.5, Cuda 9.0, CuDNN 7.6, Nvidia Drive v.384.130, tensorflow 1.12.0)
* Implemented naive model: prediction == actual head position (x, y equirectangular)
* Started add_scene_cut_column to add scene cut information to dataset
* Read about functional API to create MIMO models
* used Equirec2Perspec repository to get current viewport image

* Issues:

* Some videos start with closed eyes (confidence == 0) --> x and y data are wrong,

so idea is to delete the first rows until confidence == 1 and make difference to put x and y
in the middle of equirectangular projection

* Read arff easy with scipy.io.arff, but no dump function to save the updated arff
* Question: how to integrate saliency maps

## Week 4 (19/07 - 26/07)

* Changed LSTM to multiple input, individual LSTM for x and y
* Trained LSTM
* Used Deepgaze II on image generated by Equirec2Perspec to get comfortable with its use

## Week 5 (26/07 - 02/08)

* Followed this [LSTM tutorial](https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/)
* Save preprocessed data into .npy files
* Save viewport image and saliency map of every frame of every video to access it fast in training

* Issues:

* ResourceExhaustedError when deepgaze ii images batch size too big (20)
* Time: generate viewport and saliency map 4.5 min per video (50*18*4.5 min = 67.5 hours)
* LSTM x y: 2.5 hours per epoch, although not many layers, loss and validation_loss do not descend
* Model structure: no details in related papers

## Week 6 (02/07 - 09/08)

* Converted x_eq, y_eq into cartesian (x, y, z) to avoid jumps in data
* Used model of Paper "Your Attention is Unique: Detecting 360-Degree Video Saliency in Head-Mounted Display for Head Movement Prediction"
  * input: saliency map of the field of view, Gaussian kernel applied on x, y head position coordinates (360 head map) (12*12 px)
* Created two baseline models:
  * Sequential LSTM and Convolution with head position
  * Sequential LSTM and Convolution with head * gaze position
* Read about seq2seq recurrent models for time series prediction (encoder-decoder model)

* TODO:

* Use saliency maps with late fusion:
  * Sequential LSTM and Convolution with head position and maximum saliency point (1)
  * Sequential LSTM and Convolution with head position and weighted average between peaks saliency point (2)
* Use saliency maps with early fusion:
  * 2 channels -> 360 saliency map + 360 head map -> generate future head map (3)
  * 1 channel -> 360 saliency map + 360 head map element-wise multiplication -> generate future head map (4)
  * viewport saliency: similar to panosalnet, with more layers (5)
* Hyperparameter tuning: more/less layers, LSTM/GRU
* Create generator function to avoid model.fit in loop

* Issues:

* in Paper "Realtime 3D 360-Degree Telepresence With Deep-Learning-Based Head-Motion Prediction" - Conv1D or Conv2D?
* How to give coordinates of late fusion since relative to viewport
* Training takes too much time with the whole dataset - maybe generator improves performance
